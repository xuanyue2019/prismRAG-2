# Agent Lightning å¼ºåŒ–å­¦ä¹ ç¤ºä¾‹

æœ¬ç›®å½•åŒ…å«ä½¿ç”¨ Agent Lightning è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„å®Œæ•´ç¤ºä¾‹ã€‚

## ğŸ¯ ç¤ºä¾‹åœºæ™¯

### 1. æ–‡æœ¬æ¸¸æˆæ™ºèƒ½ä½“ (TextGameAgent)
- **ç¯å¢ƒ**: åŸºäºæ–‡æœ¬çš„å†’é™©æ¸¸æˆ
- **ä»»åŠ¡**: æ¢ç´¢ç¯å¢ƒã€æ”¶é›†ç‰©å“ã€å®Œæˆä»»åŠ¡
- **å¥–åŠ±**: å®Œæˆä»»åŠ¡ +10ï¼Œæ”¶é›†ç‰©å“ +2ï¼Œæ— æ•ˆåŠ¨ä½œ -1

### 2. å¯¹è¯åŠ©æ‰‹ (DialogueAgent)  
- **ç¯å¢ƒ**: å¤šè½®å¯¹è¯ç³»ç»Ÿ
- **ä»»åŠ¡**: æä¾›æœ‰å¸®åŠ©ã€å‡†ç¡®çš„å›ç­”
- **å¥–åŠ±**: ç”¨æˆ·æ»¡æ„åº¦ +5ï¼Œç›¸å…³å›ç­” +3ï¼Œé”™è¯¯å›ç­” -2

### 3. ä»£ç ç”Ÿæˆæ™ºèƒ½ä½“ (CodeGenAgent)
- **ç¯å¢ƒ**: ç¼–ç¨‹ä»»åŠ¡ç¯å¢ƒ
- **ä»»åŠ¡**: æ ¹æ®éœ€æ±‚ç”Ÿæˆæ­£ç¡®çš„ä»£ç 
- **å¥–åŠ±**: é€šè¿‡æµ‹è¯• +10ï¼Œè¯­æ³•æ­£ç¡® +3ï¼Œç¼–è¯‘é”™è¯¯ -5

## ğŸ“ æ–‡ä»¶ç»“æ„

```
examples/rl_training/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ requirements.txt             # è®­ç»ƒé¢å¤–ä¾èµ–
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ text_game_config.yaml   # æ–‡æœ¬æ¸¸æˆé…ç½®
â”‚   â”œâ”€â”€ dialogue_config.yaml    # å¯¹è¯åŠ©æ‰‹é…ç½®
â”‚   â””â”€â”€ codegen_config.yaml     # ä»£ç ç”Ÿæˆé…ç½®
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ text_game_env.py        # æ–‡æœ¬æ¸¸æˆç¯å¢ƒ
â”‚   â”œâ”€â”€ dialogue_env.py         # å¯¹è¯ç¯å¢ƒ
â”‚   â””â”€â”€ codegen_env.py          # ä»£ç ç”Ÿæˆç¯å¢ƒ
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ base_agent.py           # åŸºç¡€æ™ºèƒ½ä½“ç±»
â”‚   â”œâ”€â”€ text_game_agent.py      # æ–‡æœ¬æ¸¸æˆæ™ºèƒ½ä½“
â”‚   â”œâ”€â”€ dialogue_agent.py       # å¯¹è¯æ™ºèƒ½ä½“
â”‚   â””â”€â”€ codegen_agent.py        # ä»£ç ç”Ÿæˆæ™ºèƒ½ä½“
â”œâ”€â”€ trainers/
â”‚   â”œâ”€â”€ ppo_trainer.py          # PPO è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ a2c_trainer.py          # A2C è®­ç»ƒå™¨
â”‚   â””â”€â”€ reward_shaping.py       # å¥–åŠ±å¡‘é€ å·¥å…·
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_text_game.py      # æ–‡æœ¬æ¸¸æˆè®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_dialogue.py       # å¯¹è¯è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_codegen.py        # ä»£ç ç”Ÿæˆè®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ evaluate_agent.py       # æ™ºèƒ½ä½“è¯„ä¼°è„šæœ¬
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ text_game_scenarios/    # æ–‡æœ¬æ¸¸æˆåœºæ™¯æ•°æ®
â”‚   â”œâ”€â”€ dialogue_datasets/      # å¯¹è¯æ•°æ®é›†
â”‚   â””â”€â”€ coding_problems/        # ç¼–ç¨‹é—®é¢˜é›†
â””â”€â”€ utils/
    â”œâ”€â”€ data_loader.py          # æ•°æ®åŠ è½½å·¥å…·
    â”œâ”€â”€ metrics.py              # è¯„ä¼°æŒ‡æ ‡
    â””â”€â”€ visualization.py        # è®­ç»ƒå¯è§†åŒ–
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…é¢å¤–ä¾èµ–

```bash
pip install -r examples/rl_training/requirements.txt
```

### 2. è®­ç»ƒæ–‡æœ¬æ¸¸æˆæ™ºèƒ½ä½“

```bash
# åŸºæœ¬è®­ç»ƒ
python examples/rl_training/scripts/train_text_game.py \
  --config examples/rl_training/config/text_game_config.yaml \
  --epochs 100 \
  --batch-size 32

# ä½¿ç”¨ GPU è®­ç»ƒ
python examples/rl_training/scripts/train_text_game.py \
  --config examples/rl_training/config/text_game_config.yaml \
  --device cuda \
  --epochs 200

# ç»§ç»­è®­ç»ƒç°æœ‰æ¨¡å‹
python examples/rl_training/scripts/train_text_game.py \
  --config examples/rl_training/config/text_game_config.yaml \
  --resume checkpoint.pth \
  --epochs 50
```

### 3. è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½

```bash
# è¯„ä¼°æ–‡æœ¬æ¸¸æˆæ™ºèƒ½ä½“
python examples/rl_training/scripts/evaluate_agent.py \
  --agent text_game \
  --model-path models/text_game_agent.pth \
  --episodes 10

# è¯„ä¼°å¯¹è¯æ™ºèƒ½ä½“
python examples/rl_training/scripts/evaluate_agent.py \
  --agent dialogue \
  --model-path models/dialogue_agent.pth \
  --test-data data/dialogue_datasets/test.json

# ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
python examples/rl_training/scripts/evaluate_agent.py \
  --agent codegen \
  --model-path models/codegen_agent.pth \
  --output report.html
```

## âš™ï¸ é…ç½®è¯´æ˜

### è®­ç»ƒé…ç½® (YAML æ ¼å¼)

```yaml
# è®­ç»ƒå‚æ•°
training:
  algorithm: "ppo"           # ç®—æ³•: ppo, a2c, dqn
  learning_rate: 0.0001      # å­¦ä¹ ç‡
  gamma: 0.99                # æŠ˜æ‰£å› å­
  clip_epsilon: 0.2          # PPO è£å‰ªå‚æ•°
  entropy_coef: 0.01         # ç†µç³»æ•°

# ç¯å¢ƒå‚æ•°
environment:
  max_steps: 1000            # æœ€å¤§æ­¥æ•°
  reward_scale: 1.0          # å¥–åŠ±ç¼©æ”¾
  difficulty: "medium"       # éš¾åº¦çº§åˆ«

# æ¨¡å‹å‚æ•°
model:
  hidden_size: 512           # éšè—å±‚å¤§å°
  num_layers: 3              # å±‚æ•°
  dropout: 0.1               # Dropout ç‡

# ç›‘æ§å‚æ•°
monitoring:
  log_interval: 10           # æ—¥å¿—é—´éš”
  save_interval: 100         # ä¿å­˜é—´éš”
  eval_interval: 50          # è¯„ä¼°é—´éš”
```

## ğŸ® ç¯å¢ƒæ¥å£

### åŸºç¡€ç¯å¢ƒç±»

```python
class BaseRLEnv:
    def reset(self):
        """é‡ç½®ç¯å¢ƒï¼Œè¿”å›åˆå§‹çŠ¶æ€"""
        pass
        
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å› (next_state, reward, done, info)"""
        pass
        
    def get_state(self):
        """è·å–å½“å‰çŠ¶æ€"""
        pass
        
    def render(self):
        """æ¸²æŸ“ç¯å¢ƒçŠ¶æ€"""
        pass
```

### æ–‡æœ¬æ¸¸æˆç¯å¢ƒç¤ºä¾‹

```python
class TextGameEnv(BaseRLEnv):
    def __init__(self, scenario_file):
        self.scenario = load_scenario(scenario_file)
        self.current_room = self.scenario.start_room
        self.inventory = []
        self.score = 0
        self.steps = 0
        
    def reset(self):
        self.current_room = self.scenario.start_room
        self.inventory = []
        self.score = 0
        self.steps = 0
        return self._get_state()
        
    def step(self, action):
        # è§£æå’Œæ‰§è¡ŒåŠ¨ä½œ
        reward = self._execute_action(action)
        done = self._check_termination()
        next_state = self._get_state()
        info = {"score": self.score, "steps": self.steps}
        
        return next_state, reward, done, info
```

## ğŸ¤– æ™ºèƒ½ä½“æ¶æ„

### åŸºç¡€æ™ºèƒ½ä½“ç±»

```python
class BaseAgent(nn.Module):
    def __init__(self, state_size, action_size, hidden_size=512):
        super().__init__()
        self.state_encoder = nn.Linear(state_size, hidden_size)
        self.action_policy = nn.Linear(hidden_size, action_size)
        self.value_net = nn.Linear(hidden_size, 1)
        
    def forward(self, state):
        hidden = F.relu(self.state_encoder(state))
        action_probs = F.softmax(self.action_policy(hidden), dim=-1)
        state_value = self.value_net(hidden)
        return action_probs, state_value
        
    def act(self, state):
        with torch.no_grad():
            action_probs, _ = self.forward(state)
            action = torch.multinomial(action_probs, 1).item()
        return action
```

## ğŸ“Š è®­ç»ƒæµç¨‹

### PPO è®­ç»ƒå¾ªç¯

```python
def train_ppo(agent, env, config):
    optimizer = optim.Adam(agent.parameters(), lr=config.learning_rate)
    
    for episode in range(config.episodes):
        state = env.reset()
        episode_reward = 0
        
        states, actions, rewards, dones = [], [], [], []
        
        for step in range(config.max_steps):
            action = agent.act(state)
            next_state, reward, done, _ = env.step(action)
            
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            dones.append(done)
            
            state = next_state
            episode_reward += reward
            
            if done:
                break
                
        # PPO æ›´æ–°
        advantages = compute_advantages(rewards, dones, agent, config.gamma)
        loss = compute_ppo_loss(agent, states, actions, advantages, config.clip_epsilon)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

### æ€§èƒ½æŒ‡æ ‡
- **å¹³å‡å¥–åŠ±**: æ¯ä¸ªå›åˆçš„å¹³å‡ç´¯ç§¯å¥–åŠ±
- **æˆåŠŸç‡**: å®Œæˆä»»åŠ¡çš„æ¯”ä¾‹
- **æ­¥æ•°æ•ˆç‡**: å®Œæˆä»»åŠ¡çš„å¹³å‡æ­¥æ•°
- **æ¢ç´¢ç‡**: è®¿é—®ä¸åŒçŠ¶æ€çš„æ¯”ä¾‹

### è®­ç»ƒç›‘æ§
```bash
# è®­ç»ƒè¿›åº¦ç¤ºä¾‹
Epoch: 50/100 | Reward: 15.2 Â± 3.1 | Success: 72% | Steps: 45.3
Epoch: 100/100 | Reward: 28.7 Â± 2.5 | Success: 92% | Steps: 32.1
```

## ğŸ¯ é«˜çº§åŠŸèƒ½

### è¯¾ç¨‹å­¦ä¹ 
```yaml
curriculum:
  enabled: true
  levels:
    - difficulty: "easy"
      scenarios: ["beginner_*.json"]
      min_success: 0.8
    - difficulty: "medium" 
      scenarios: ["intermediate_*.json"]
      min_success: 0.7
    - difficulty: "hard"
      scenarios: ["expert_*.json"]
      min_success: 0.6
```

### é›†æˆå­¦ä¹ 
```python
# å¤šä¸ªæ™ºèƒ½ä½“é›†æˆ
ensemble_agents = [
    load_agent("models/agent1.pth"),
    load_agent("models/agent2.pth"), 
    load_agent("models/agent3.pth")
]

def ensemble_act(state):
    actions = [agent.act(state) for agent in ensemble_agents]
    return max(set(actions), key=actions.count)
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å¥–åŠ±ä¸æ”¶æ•›**
   - è°ƒæ•´å¥–åŠ±ç¼©æ”¾
   - æ£€æŸ¥å¥–åŠ±å‡½æ•°è®¾è®¡
   - å¢åŠ ç†µç³»æ•°é¼“åŠ±æ¢ç´¢

2. **è®­ç»ƒä¸ç¨³å®š**
   - å‡å°å­¦ä¹ ç‡
   - å¢åŠ æ‰¹é‡å¤§å°
   - ä½¿ç”¨æ¢¯åº¦è£å‰ª

3. **è¿‡æ‹Ÿåˆ**
   - å¢åŠ  Dropout
   - ä½¿ç”¨æ—©åœç­–ç•¥
   - å¢åŠ æ­£åˆ™åŒ–

## ğŸ“ ä¸‹ä¸€æ­¥

1. **è‡ªå®šä¹‰ç¯å¢ƒ**: ä¿®æ”¹ `environments/` ä¸­çš„ç±»åˆ›å»ºæ–°ç¯å¢ƒ
2. **è°ƒæ•´é…ç½®**: ä¿®æ”¹ YAML é…ç½®æ–‡ä»¶ä¼˜åŒ–è®­ç»ƒå‚æ•°
3. **æ·»åŠ ç›‘æ§**: é›†æˆ TensorBoard æˆ– WandB è¿›è¡Œå¯è§†åŒ–
4. **éƒ¨ç½²ç”Ÿäº§**: ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå®é™…åº”ç”¨

---

**ç»´æŠ¤å›¢é˜Ÿ**: å¼ºåŒ–å­¦ä¹ ç ”ç©¶ç»„  
**æœ€åæ›´æ–°**: 2025-08-19